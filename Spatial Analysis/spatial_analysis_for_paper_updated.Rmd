---
title: "Spatial Analysis for Paper"
author: "Michelle Bang"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Part 0: Loading all of my files and functions

```{r}
source("../LabWorkNathan-master/R/xml_to_coordinates.R")
source("../LabWorkNathan-master/R/coordinates_to_xbins.R")
source("../LabWorkNathan-master/R/xml_to_coord_edited.R")
library(ggplot2)
library(tidyverse)
library(ggpubr)
```

```{r}
library(qvalue)
library(scales)
```

```{r}
xml_to_coord <- function(input_data_path){
  #print(input_data_path)
  if (grepl("\\_inference.xml", input_data_path)){
    xml_to_coordinates2(input_data_path)
  } else{
    xml_to_coordinates(input_data_path)
  }
}
```

```{r}
list_alleles <- list.dirs("/Users/michellebang/st research lab/spatialanalysis/SpatialAnalysisMichelle/SpatialAnalysis_AllelesForPaper_new", full.names = FALSE, recursive = FALSE)
```

```{r}
filenames = vector(mode='list', length= length(list_alleles))
```

```{r}
for (i in 1:length(list_alleles)){
  filenames[[i]] = vector(mode='list', length= 3)
}
```

```{r}
for (i in 1:length(list_alleles)){
  filenames[[i]][[1]] <- list_alleles[[i]] #Getting name of allele
  filenames[[i]][[2]] = list.files(paste0("/Users/michellebang/st research lab/spatialanalysis/SpatialAnalysisMichelle/SpatialAnalysis_AllelesForPaper_new/", list_alleles[[i]]), pattern="*.xml", full.names=TRUE) #Getting path to observation.
  filenames[[i]][[3]] = as.list(basename(filenames[[i]][[2]])) #Extracting the name of obs.
}
```

```{r}
#truePVal = data.frame(matrix(nrow = length(filenames), ncol = 3)) 
#colnames(truePVal) <- c("Allele", "Linear", "Quad")
#truePVal[1] = list_alleles
truePVal = data.frame(matrix(nrow = length(filenames), ncol = 13)) 
colnames(truePVal) <- c("Allele", "True Prop", "Lin Prop", "Quad Prop", "Lin Prop Count", "Quad Prop Count", "Adj P-Val Prop (< .1)", "Adj P-Val (< .1) from Lin", "Adj P-Val (<.1) from Quad", "Total Count", "SE", "Lower 95% CI", "Upper 95% CI")
truePVal[1] = list_alleles
```

```{r}
megaDf = data.frame()
anova_comp_full = data.frame()
megaBestDf = data.frame()
```

```{r}
not_able <- function (){
  "NA"
}
```

```{r}
stat_sum = data.frame(matrix(nrow = length(filenames), ncol = 7)) 
colnames(stat_sum) <- c("Allele", "Lin Coef", "Lin SE", "Lin Var", "Quad Coef", "Quad SE", "Quad Var")
stat_sum[1] = list_alleles
```

```{r}
for (n in 1:length(filenames)){
  coords = lapply(filenames[[n]][[2]], xml_to_coord)

  #Removes faulty xml from the files. 

  sequence = rev(seq(1:length(coords)))

#Checks if there will be an error when coordinates_to_xbins runs. If there is, it removes it from filenames 
for (i in sequence) {
  if (nrow(coords[[i]]) == 0)  {
    filenames[[n]][[2]] = filenames[[n]][[2]][-i]
    filenames[[n]][[3]] = filenames[[n]][[3]][-i]
  }
}

#The 0 observation issue is caused by files that don't use 1 and 2 as their marker numbers which is a requirement for xml_to_coordinates
bin_data = lapply(coords, function(x) {coordinates_to_xbins(x,16)} )

sequence = rev(seq(1:length(bin_data)))

#Removes the files that break the glm, namely the ears with only one kind of kernel allele. At most removes the cases with very few WT or GFP which are by definition odd ears. 

for (i in sequence) {
  sumWT = sum(bin_data[[i]]$WT)
  sumGFP = sum(bin_data[[i]]$GFP)
    if ((sumWT < 5) | (sumGFP < 5)) {
      bin_data = bin_data[-i]
      filenames[[n]][[2]] = filenames[[n]][[2]][-i]
      filenames[[n]][[3]] = filenames[[n]][[3]][-i]
    }
}

#The 0 observation issue is caused by files that don't use 1 and 2 as their marker numbers which is a requirement for xml_to_coordinates
full_data = lapply(coords, function(x) {coordinates_to_xbins(x,1)} )

sequence = rev(seq(1:length(full_data)))

#Removes the files that break the glm, namely the ears with only one kind of kernel allele. At most removes the cases with very few WT or GFP which are by definition odd ears. 

for (i in sequence) {
  sumWT = sum(full_data[[i]]$WT)
  sumGFP = sum(full_data[[i]]$GFP)
    if ((sumWT < 5) | (sumGFP < 5)) {
      full_data = full_data[-i]
    }
}


TransmissionRate <- c()
for (i in 1:length(filenames[[n]][[2]])) {
  TransmissionRate <- append(TransmissionRate, full_data[[i]]$GFP/(full_data[[i]]$GFP + full_data[[i]]$WT))
}

#Start chopping off the ends
sequence = seq(1,length(bin_data))

#Creating an empty list
bin_ed = list()

for (i in sequence) {
    bin_ed[[i]] = data.frame("bins" = bin_data[[i]]$bins[2:15], "WT" = bin_data[[i]]$WT[2:15] , "GFP" = bin_data[[i]]$GFP[2:15])
}

#Turn the data into a glm
ends_glm_data = lapply(bin_ed, function(x) {glm(cbind(GFP,WT) ~ bins,family = quasibinomial(link = "logit"), data = x)})


#Extract the p values
ends_pv_data = lapply(ends_glm_data, function(x) {(summary(x)$coefficients)})

ends_p_data = lapply(ends_pv_data, function(x) {x[8]})

#Comment this out to look at the non adjusted values, you can swap the method by changing 'method = fdr'
adj_p_data = p.adjust(ends_p_data,method = "fdr")

adj_p_df <- data.frame(matrix(unlist(adj_p_data), nrow=length(adj_p_data), byrow=T))
names(adj_p_df)[names(adj_p_df) == colnames(adj_p_df[1])] = "adj_p_value"

ends_p_df <- data.frame(matrix(unlist(ends_p_data), nrow=length(ends_p_data), byrow=T))
names(ends_p_df)[names(ends_p_df) == colnames(ends_p_df[1])] = "p_value"

titleDf <- data.frame(unlist(filenames[[n]][[3]]))
colnames(titleDf) <- "Name"

endsfullDf = cbind(titleDf, ends_p_df)
endsfullDf <- cbind(endsfullDf, TransmissionRate)
endsfullDf <- cbind(endsfullDf, adj_p_df)

endsfullDf <- endsfullDf %>%
  mutate(allele = filenames[[n]][[1]])

#Uncomment to save data for just linear term as .tsv
#endsfullDf1 = as.matrix(endsfullDf)
#write.table(endsfullDf1, file = paste0(filenames[[n]][[1]],"_glm_ends.tsv"), sep = "\t", row.names=FALSE) 

# TEST FOR PROP OF TRUE NULL P-VALUES

#Another way of accounting for multiple testing
#Should give out percentage of null p-values (we are interested in 1-nullper)

min_p = min(unlist(ends_p_data)) + .05
per_adj_p_data = pi0est(unlist(ends_p_data), lambda = seq(min_p, .5, length.out=10), pi0.method = "bootstrap")
trueProp = 1-per_adj_p_data$pi0
truePVal[n,3] = trueProp


#ANOVA Testing for quadratic term

#First part of set up should be the same as GLM with ends chopped off.


bins_squared_ends = bin_ed
bin_sq_data_ends = bin_ed

sequence = seq(1, length(bin_ed))
sequence2  = seq(1, 14)

for (i in sequence) {
  for (j in sequence2) {
  bins_squared_ends[[i]]$bins[j] = (bin_sq_data_ends[[i]]$bins[j])^2
  }
  bin_sq_data_ends[[i]] = data.frame(bin_sq_data_ends[[i]],bins_squared_ends[[i]]$bins)
}

#Turn the data into a glm
#If this breaks just double check how R is naming the bin_squared column of bin_data and replace "bins_squared..i...bins" with that
sq_glm_data_ends = lapply(bin_sq_data_ends, function(x) {glm(cbind(GFP,WT) ~ bins + bins_squared_ends..i...bins,family = quasibinomial(link = "logit"), data = x)})

#Creating null model
null_ends_model <- lapply(bin_ed, function(x) {glm(cbind(GFP,WT) ~ 1,family = quasibinomial(link = "logit"), data = x)})


#Setting up empty list
anova_ends_data <- vector(length = length(bin_ed))

for (i in 1:length(bin_ed)){
  anova_ends_data[i] <- anova(null_ends_model[[i]], sq_glm_data_ends[[i]], test = "F")$"Pr(>F)"[2]
}

anova_ends_df <- as.data.frame(anova_ends_data)
row.names(anova_ends_df) <- 1:length(bin_ed)
adj_anova = p.adjust(anova_ends_data, method = "fdr")
colnames(anova_ends_df)[1] = "p_value_sq"
#Uncomment for when you want to add column for names.
#anova_ends_df <- cbind(anova_ends_df, titleDf)
#colnames(anova_ends_df)[2] = "name"
anova_ends_df <- cbind(anova_ends_df, adj_anova)
colnames(anova_ends_df)[2] = "adj_p_value_sq"

#Uncomment if you want to save just ANOVA data as .tsv
#write.table(anova_ends_df, file = paste0(filenames[[n]][[1]],"_anova_ends.tsv"), sep = "\t", row.names=FALSE) 

# TEST FOR PROP OF TRUE NULL P-VALUES

#Another way of accounting for multiple testing
#Should give out percentage of null p-values (we are interested in 1-nullper)
min_p = min(unlist(anova_ends_data)) + .05
per_adj_anova = pi0est(unlist(anova_ends_data), lambda = seq(min_p, .5, length.out = 10), pi0.method = "bootstrap")
truePropA = 1-per_adj_anova$pi0
truePVal[n,4] = truePropA

endsfullDf <- cbind(endsfullDf, anova_ends_df)

#Saving all data as .tsv
megaDf <- rbind(megaDf, endsfullDf)

coef_ends_glm <- c()
for(k in 1:length(ends_glm_data)){
  coef_ends_glm <- append(coef_ends_glm, ends_glm_data[[k]]$coefficients[["bins"]])
}
stat_sum[n,2] = mean(coef_ends_glm) 

se_ends <- c()
for(k in 1:length(ends_glm_data)){
  se_ends <- append(se_ends, summary(ends_glm_data[[k]])$coefficients[, 2][["bins"]])
}
se_ends_new <- se_ends^2
stat_sum[n,3] = sqrt(sum(se_ends_new)/length(ends_glm_data)^2) 

#var_ends <- c()
#just take the variance of the coefficients
#for(k in 1:length(ends_glm_data)){
#  var_ends <- append(var_ends, vcov(ends_glm_data[[k]])[2,2])
#}
stat_sum[n,4] = var(coef_ends_glm)

#stat_sum[n,4] = sum(var_ends)/length(ends_glm_data)^2 

coef_ends_sq_glm <- c()

for(k in 1:length(sq_glm_data_ends)){
  coef_ends_sq_glm <- append(coef_ends_sq_glm, sq_glm_data_ends[[k]]$coefficients[["bins_squared_ends..i...bins"]])
}
stat_sum[n,5] = mean(coef_ends_sq_glm)

se_sq_ends <- c()
for(k in 1:length(sq_glm_data_ends)){
  se_sq_ends <- append(se_sq_ends, summary(sq_glm_data_ends[[k]])$coefficients[, 2][["bins"]])
}
se_sq_ends_new <- se_sq_ends^2
stat_sum[n,6] = sqrt(sum(se_sq_ends_new)/length(sq_glm_data_ends)^2) 

#var_sq_ends <- c()
#for(k in 1:length(sq_glm_data_ends)){
#  var_sq_ends <- append(var_sq_ends, vcov(sq_glm_data_ends[[k]])[2,2])
#}
#stat_sum[n,7] = sum(var_sq_ends)/length(sq_glm_data_ends)^2
stat_sum[n,7] = var(coef_ends_sq_glm)

#ANOVA to test between linear model and the quadratic model
#Setting up empty list
anova_comparison <- vector(length = length(bin_ed))

for (i in 1:length(bin_ed)){
  anova_comparison[i] <- anova(ends_glm_data[[i]], sq_glm_data_ends[[i]], test = "F")$"Pr(>F)"[2]
}

anova_comp_df <- as.data.frame(anova_comparison)
row.names(anova_comp_df) <- 1:length(bin_ed)
adj_anova_comp = p.adjust(anova_comparison, method = "fdr")
colnames(anova_comp_df)[1] = "p_value"
anova_comp_df <- cbind(anova_comp_df, adj_anova_comp)
colnames(anova_comp_df)[2] = "adj_p_value"
anova_comp_df <- anova_comp_df %>%
  mutate(allele = filenames[[n]][[1]])

anova_comp_full <- rbind(anova_comp_full, anova_comp_df)

#Choosing only one p-value for pi0est
pi0est_p_vals <- data.frame(matrix(nrow = length(bin_ed), ncol = 1)) 
best_df <- data.frame(matrix(nrow = length(bin_ed), ncol = 8))
colnames(best_df) <- c("Allele", "Name", "P_Value", "Model", "Adj_P_Value", "Lin_Coef", "Quad_Coef", "Inc_Dec")
best_df[1] = filenames[[n]][[1]]
best_df[2] = titleDf

quad_count = 0
lin_count = 0

for (i in 1:length(bin_ed)) {
  if (anova_comp_df[[1]][[i]] > .1) {
    pi0est_p_vals[[1]][[i]] = ends_p_df[[1]][[i]]
    best_df[[3]][[i]] = ends_p_df[[1]][[i]]
    best_df[[4]][[i]] = "L"
    #add line about choosing adj pvalue for quad or lin
    lin_count = lin_count + 1
    #getting coef
    best_df[[6]][[i]] = coef_ends_glm[[i]]
    best_df[[7]][[i]] = "NA"
  } else {
    pi0est_p_vals[[1]][[i]] = anova_ends_df[[1]][[i]]
    best_df[[3]][[i]] = anova_ends_df[[1]][[i]]
    best_df[[4]][[i]] = "Q"
    #add line about choosing adj pvalue for quad or lin
    quad_count = quad_count + 1
    #getting coef
    best_df[[6]][[i]] = coef_ends_glm[[i]]
    best_df[[7]][[i]] = coef_ends_sq_glm[[i]]
  }
}


min_p = min(unlist(pi0est_p_vals)) + .05
per_adj_p_data = pi0est(unlist(pi0est_p_vals), lambda = seq(min_p, .5, length.out=10), pi0.method = "bootstrap")
trueProp = 1-per_adj_p_data$pi0
nullProp = per_adj_p_data$pi0
pi_lamba = per_adj_p_data$pi0.lambda
lambda = per_adj_p_data$lambda
l = 0
for (i in 1:length(lambda)) {
  if (nullProp == pi_lamba[i]) {
    l = lambda[i]
    break
  }
}
truePVal[n,2] = trueProp
truePVal[n,5] = lin_count
truePVal[n,6] = quad_count
m = lin_count + quad_count
SE <- sqrt((nullProp * (1-nullProp*(1-l)))/(m*(1-l)))
truePVal[n,11] = SE
truePVal[n,12] = trueProp - 1.96*SE
truePVal[n,13] = trueProp + 1.96*SE

#prop of adj pvalue below .1
adj_p_comp = p.adjust(unlist(pi0est_p_vals), method = "fdr")
adj_p_comp_df <- data.frame(matrix(unlist(adj_p_comp), nrow=length(adj_p_comp), byrow=T))
names(adj_p_comp_df)[names(adj_p_comp_df) == colnames(adj_p_comp_df[1])] = "adj_p_value"

best_df[[5]] = adj_p_comp

adj_cnt <- adj_p_comp_df %>%
  filter(adj_p_value < .1) %>%
  summarize(count = n())

adj_l_cnt <- best_df %>%
  filter(Adj_P_Value < .1, Model == "L") %>%
  summarize(count = n())

adj_q_cnt <- best_df %>%
  filter(Adj_P_Value < .1, Model == "Q") %>%
  summarize(count = n())

truePVal[n,7] = adj_cnt[[1]][[1]]/(lin_count + quad_count)

truePVal[n,8] = adj_l_cnt[[1]][[1]]

truePVal[n,9] = adj_q_cnt[[1]][[1]]

#total count
truePVal[n,10] = lin_count + quad_count

#Looking at the specific ears with small adj p_values, check if the trend is increasing or decreasing
for (i in 1:length(bin_ed)) {
  if (best_df[[5]][[i]] < .1) {
    if (best_df[[4]][[i]] == "L") {
      if (best_df[[6]][[i]] > 0) {
        best_df[[8]][[i]] = "I"
      } else {
        best_df[[8]][[i]] = "D"
      }
    } else {
      best_df[[8]][[i]] = "Check"
    }
  } else {
    best_df[[8]][[i]] = "NA"
  }
}

megaBestDf <- rbind(megaBestDf, best_df)
}

```


```{r}
#looking at the specific ears with small adj pvalues, check if the trend is increasing or decreasing 
#for lin, just need to look at the coefficient
#for quad, need an extra step to check if its monotonic
```

```{r}
write.table(truePVal, file = "prop_true_pval_v050924.tsv", sep = "\t", row.names=FALSE) 
```

```{r}
#write.table(megaBestDf, file = "incr_decr.tsv", sep = "\t", row.names=FALSE) 
```

```{r}
megaBestDf %>%
  filter(Inc_Dec == "Check")
```
```{r}
sig_obs <- megaBestDf %>%
  filter(Inc_Dec != "NA")
```

```{r}
write.table(sig_obs, file = "significant_observations.tsv", sep = "\t", row.names=FALSE)
```

```{r}
#orderedMegaDf <- megaDf[,c(5,1,2,3,4,6,7)]
```

```{r}
#write.table(megaDf, file = "spatial_analysis_for_paper_updated.tsv", sep = "\t", row.names=FALSE)
```

```{r}
new_df <- read.table(file = "spatial_analysis_for_paper_updated.tsv",
                              header = TRUE,
                              sep = '\t', na.strings = c("", " ", NA))
```

```{r}
p_lin_cnt <- new_df %>%
  filter(p_value < .05) %>%
  group_by(allele) %>%
  summarize(p_val_count = n())
```
```{r}
p_sq_cnt <-new_df %>%
  filter(p_value_sq < .05) %>%
  group_by(allele) %>%
  summarize(p_val_sq_count = n())
```

```{r}
cnt <-new_df %>%
  group_by(allele) %>%
  summarize(count = n())
```

```{r}
#write.table(stat_sum, file = "stat_sum_paper.tsv", sep = "\t", row.names=FALSE)
```

```{r}
#write.table(p_lin_cnt, file = "spa_p_lin_count.tsv", sep = "\t", row.names=FALSE)
#write.table(cnt, file = "spa_count.tsv", sep = "\t", row.names=FALSE)
#write.table(p_sq_cnt, file = "spa_p_sq_count.tsv", sep = "\t", row.names=FALSE)
```

```{r}
mean_count <- new_df %>%
  group_by(allele) %>%
  summarize(mean_rate = mean(TransmissionRate),
            count = n()) %>%
  arrange(desc(mean_rate))

```

```{r}
#write.table(mean_count, file = "mean_count_paper.tsv", sep = "\t", row.names=FALSE)
```

```{r}
anova_comp <- anova_comp_full %>%
  group_by(allele) %>%
  summarize(mean_p = mean(p_value), 
            var_p = var(p_value),
            max_p = max(p_value),
            min_p = min(p_value), 
            mean_adj = mean(adj_p_value),
            )
```

```{r}
#write.table(anova_comp, file = "anova_comp_paper.tsv", sep = "\t", row.names=FALSE)
```

```{r}
#write.table(anova_comp_full, file = "anova_comp_full_paper.tsv", sep = "\t", row.names=FALSE)
```
